# Practice-for-LLM

- **ç¬”è®°**

  - [å“ˆå·¥å¤§ã€ŠChatGPTè°ƒç ”æŠ¥å‘Šã€‹ç¬”è®°](å¤§æ¨¡å‹æ¢³ç†/å“ˆå·¥å¤§ã€ŠChatGPTè°ƒç ”æŠ¥å‘Šã€‹ç¬”è®°.md)

  - [**GPT3**](å¤§æ¨¡å‹æ¢³ç†/GPT3.md)

  - [InstructGPT](å¤§æ¨¡å‹æ¢³ç†/InstructGPT.md)


- **è®ºæ–‡**
- [ ]  **A Survey of Large Language Models** ğŸ’¥
- [x]  **GPTç ”ç©¶æŠ¥å‘Š**
- [x]  GPT1, 2, 3, 4åŸæ–‡
    - [x]  **Improving language understanding by generative pre-training.**
    - [x]  **Language models are unsupervised multitask learners.**
    - [x]  **Language Models are Few-Shot Learners**
    - [ ]  **GPT-4 Technical Report**
- [x]  InstructGPT ****Training language models to follow instructions with human feedback****
- [x]  Self-Instruct ****Self-Instruct: Aligning Language Model with Self Generated Instructions****
- [x]  LLaMA ****LLaMA: Open and Efficient Foundation Language Models****
- [x]  Alpaca ****Alpaca: A Strong, Replicable Instruction-Following Model****
- [x]  BLOOM å’Œ BLOOMZ ****BLOOM: A 176B-Parameter Open-Access Multilingual Language Model å†™çš„æ¯”è¾ƒè¯¦ç»†ï¼Œå¯ä»¥ä½œä¸ºæŒ‡å—****
- [ ]  OPT å’Œ OPT-IML  **Scaling Language Model Instruction Meta Learning through the Lens of Generalization**
- [ ]  Lora **LoRA: Low-Rank Adaptation of Large Language Models**
- [ ]  GPT4æ—©æœŸå®éªŒæŠ¥å‘Š ****GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models****


**å‚æ•°è§„æ¨¡ã€æ•°æ®é›†ã€æ¨¡å‹èƒ½åŠ›çš„æ¢ç©¶**

- [ ]  **Scaling laws for neural language models**
- [ ]  **Training compute-optimal large language models**
- [ ]  LLaMA ****LLaMA: Open and Efficient Foundation Language Models****

### å·¥ç¨‹æ–¹é¢


- **æ¡†æ¶ç±»**
- [ ]  **DPï¼ŒDDPï¼ŒZeROï¼ŒFSDP**
- [ ]  **Megatron**
- [ ]  [**microsoft/DeepSpeed**](https://github.com/microsoft/DeepSpeed) ğŸ’¥
- [ ]  [**hwchase17/langchain**](https://github.com/hwchase17/langchain) åŸºäºLLMå¼€å‘åº”ç”¨çš„SDK ğŸ’¥
- [ ]  [**facebookresearch /Â fairscale**](https://github.com/facebookresearch/fairscale) å’Œdeepspeedç±»ä¼¼ï¼Œ[Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale)
- [ ]  [**hpcaitech/ColossalAI**](https://github.com/hpcaitech/ColossalAI) åŒä¸Šï¼Œä¸­æ–‡å‹å¥½
- [ ]  [**huggingface /Â peft**](https://github.com/huggingface/peft) [PEFT: åœ¨ä½èµ„æºç¡¬ä»¶ä¸Šå¯¹åäº¿è§„æ¨¡æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ](https://zhuanlan.zhihu.com/p/610503561) å®ç°äº†loraã€prefix tuningã€prompt tuningã€p-tuning
- [ ]  [**lvwerra /Â trl**](https://github.com/lvwerra/trl)  TRL - Transformer Reinforcement Learningï¼Œå®ç°äº†PPO Trainer [åœ¨ä¸€å¼  24 GB çš„æ¶ˆè´¹çº§æ˜¾å¡ä¸Šç”¨ RLHF å¾®è°ƒ 20B LLMs](https://zhuanlan.zhihu.com/p/616346543) ğŸ’¥


- **å®ç°ç±»**
- [ ]  [**karpathy/nanoGPT**](https://github.com/karpathy/nanoGPT) ğŸ’¥
- [ ]  [**yizhongw /Â self-instruct**](https://github.com/yizhongw/self-instruct) Self-Instructçš„ä»£ç 
- [ ]  [**tatsu-lab/stanford_alpaca**](https://github.com/tatsu-lab/stanford_alpaca) åŸºäºself-instrctionçš„LLaMaæŒ‡ä»¤ç²¾è°ƒ ****[standford-alpacaå¾®è°ƒè®°å½•](https://zhuanlan.zhihu.com/p/616119919)**** ğŸ’¥


- **å®è·µç±»**
- [ ]  [**tloen /Â alpaca-lora**](https://github.com/tloen/alpaca-lora) åŸºäºLoraçš„LLaMaæŒ‡ä»¤ç²¾è°ƒ ****[è®­ç»ƒä¸ªä¸­æ–‡ç‰ˆChatGPTæ²¡é‚£ä¹ˆéš¾ï¼šä¸ç”¨A100ï¼Œå¼€æºAlpaca-LoRA+RTX 4090å°±èƒ½æå®š](https://zhuanlan.zhihu.com/p/617221484)****
- [ ]  [**LC1332 /Â Chinese-alpaca-lora**](https://github.com/LC1332/Chinese-alpaca-lora) åŸºäºLoraçš„ä¸­æ–‡LLaMaæŒ‡ä»¤ç²¾è°ƒ ****[Alpaca-Lora è½»é‡çº§ ChatGPT çš„å¼€æºå®ç°](https://zhuanlan.zhihu.com/p/615646636)****
- [ ]  [**LianjiaTech /Â BELLE**](https://github.com/LianjiaTech/BELLE) ä¸­æ–‡LLaMaæŒ‡ä»¤ç²¾è°ƒ
- [ ]  [**databrickslabs /Â dolly**](https://github.com/databrickslabs/dolly) [0é—¨æ§›å…‹éš†ChatGPTï¼30åˆ†é’Ÿè®­å®Œï¼Œ60äº¿å‚æ•°æ€§èƒ½å ªæ¯”GPT-3.5](https://zhuanlan.zhihu.com/p/617345561)
- [ ]  [**LLMçœå†…å­˜æ–¹æ³•**](https://zhuanlan.zhihu.com/p/616858352)
- [ ]  ****[æ€»ç»“å¼€æºå¯ç”¨çš„Instruct/Prompt Tuningæ•°æ®](https://zhuanlan.zhihu.com/p/615277009)****
- [ ]  ****[æ€»ç»“å½“ä¸‹å¯ç”¨çš„å¤§æ¨¡å‹LLMs](https://zhuanlan.zhihu.com/p/611403556)****
- [ ]  ****[ä½¿ç”¨ DeepSpeed å’Œ Accelerate è¿›è¡Œè¶…å¿« BLOOM æ¨¡å‹æ¨ç†](https://zhuanlan.zhihu.com/p/602142554?utm_medium=social&utm_oi=46337705902080&utm_psn=1622986060279119872&utm_source=wechat_session)****
- [ ]  [**dair-ai/Prompt-Engineering-Guide**](https://github.com/dair-ai/Prompt-Engineering-Guide)Â [ä¸­æ–‡ç‰ˆ](https://github.com/wangxuqi/Prompt-Engineering-Guide-Chinese)  promptå·¥ç¨‹å¸ˆæŒ‡å— 
- [ ]  ****[å¤ç°ChatGPTçš„éš¾ç‚¹ä¸å¹³æ›¿](https://zhuanlan.zhihu.com/p/607847588)****
- [ ]  ****[å¹³æ›¿chatGPTçš„å¼€æºæ–¹æ¡ˆ](https://zhuanlan.zhihu.com/p/618926239?utm_medium=social&utm_oi=46337705902080&utm_psn=1625959298282090496&utm_source=wechat_session)****
- [ ]  **[ChatGPTçš„ä½æˆæœ¬â€œå¹³æ›¿â€å½“ä¸‹å®ç°è·¯çº¿](https://mp.weixin.qq.com/s/5SNJLLs9Hw0uvjkcLQflvA)**
- [ ]  ****[Stealing Large Language Models: å…³äºå¯¹ChatGPTè¿›è¡Œæ¨¡å‹çªƒå–çš„ä¸€äº›å·¥ä½œ](https://zhuanlan.zhihu.com/p/621179159?utm_medium=social&utm_oi=46337705902080&utm_psn=1629976091313029120&utm_source=wechat_session)****
- [ ]  **[å¾®è½¯å¼€æºDeep Speed Chatï¼Œäººäººæ‹¥æœ‰ChatGPTï¼](https://mp.weixin.qq.com/s/6y5e9MvSXXLCj-q7FI08Kw)** ğŸ’¥